---
title: "Binomial Distributions"
author: "Bret Larget"
output: html_document
---

\newcommand{\E}{\mathsf{E}}
\newcommand{\Var}{\mathsf{Var}}
\renewcommand{\prob}{\mathsf{P}}

This R Markdown document includes contributions from Professor Jessi Kehe

### Setup details

- You will need the package **tidyverse** for this file
- You will need the **egg** package for this file
    - Install **egg** if you do not already have it

- This lecture use the following scripts, assumed to be in your course scripts directory.
    - `COURSE/scripts/viridis.R`
    - `COURSE/scripts/ggprob.R`

- The file `ggprob.R` contains a number of functions for graphing probability distributions in a tidyverse-friendly manner.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE,message=FALSE,warning=FALSE,cache=TRUE,autodep=TRUE,cache.comments=FALSE)
library(tidyverse)
library(egg)
source("../../scripts/viridis.R")
source("../../scripts/ggprob.R")
```

## Binomial Distribution

- Binomial distributions are discrete distributions that consider the number of "successes" in a fixed series of $n$ trials.

- Example:  Flipping a coin 10 times and recording the number of heads (successes) would follow a binomial distribution with $n = 10$ and probability of success $p = 0.5$ (assuming the coin is fair!).

- Binomial distributions are used when we want to know about the occurrence of an event, not its magnitude.  
  - In a clinical trial, a patient’s condition may improve or not. The binomial distribution would model the number of patients who improved, not how much better they feel.  


#### The BINS acronym for binomial assumptions

A binomial random variable satisfies the following properties:

- B = **binary outcomes** (each trial may be categorized with two outcomes, conventionally labeled *success* and *failure*)
- I = **independence** (results of trials do not affect probabilities of other trial outcomes)
- N = **fixed sample size** (the sample size is pre-specified and not dependent on outcomes)
- S = **same probability** (each trial has the same probability of *success*)

## Binomial Parameters

- There are two parameters for the binomial distribution:
    - $n$ which is the number of trials
    - $p$ which is the *success* probability
    
- If $X$ follows a binomial distribution with parameters $n$ and $p$, 
we write $X \sim \text{Binomial}(n, p)$
    
    
## Binomial Moments

- The mean is $\mu = np$
- The variance is $\sigma^2 = np(1-p)$
- The standard deviation is $\sigma = \sqrt{np(1-p)}$

- We will not go over the mathematical derivation of these of these formulas, but can demonstrate them with a quick simulation.
    - We will generate 1,000,000 binomial random variables with $n = 100$ and $p = 0.5$.
    - The exact mean is $\mu = 100 \times 0.5 = 50$.
    - We expect the sample mean to be very close with such a large sample size.
    - The exact variance is $\sigma^2 = 100 \times 0.5 \times (1-0.5) = 25$.
    - Thus, the standard deviation of the distribution is $\sigma = \sqrt{25} = 5$.
    - Again, we expect the sample standard deviation to be very close to this value.
- The function `rbinom()` may be used to generate random binomial variables.
- We will keep the values in a tibble and then summarize and graph them.

```{r}
B = 1000000
n = 100
p = 0.5

binomial_example = tibble(
  x = rbinom(B, n, p)
)

## use as.data.frame to print more digits
binomial_example %>% 
  summarize(mean = mean(x),
            variance = var(x),
            sd = sd(x)) %>% 
  as.data.frame()
```

- The simulation does not prove that the formulas are correct, but is consistent with the theory.
- Try doing the same thing with different numerical values!

- We can also make a bar graph of the numbers generated in the simulation.
    - Note here setting `aes(y = ..density..)` changes the scale on the y axis to plot as a density instead of as counts.
    - Heights are adjusted so that the total area is equal to one.

```{r}
ggplot(binomial_example, aes(x=x)) +
  geom_histogram(aes(y = ..density..),
                 center = 50, binwidth = 1,
                 color = "black", fill = "firebrick") +
  ylab("Probability") +
  ggtitle("Simulated Binomial Distribution",
          subtitle = "n = 100, p = 0.5") +
  theme_minimal()
```

- An alternative is to use `geom_segment()` to plot the observed proportions.
    - Here, we compute the proportions before sending to `ggplot()`

```{r}
binomial_example %>% 
  count(x) %>% 
  ungroup() %>% 
  mutate(p = n/sum(n)) %>% 
ggplot(aes(x=x, y=p, xend=x, yend=0)) +
  geom_segment(color = "blue") +
  geom_hline(yintercept = 0) +
  ylab("Probability") +
  ggtitle("Simulated Binomial Distribution",
          subtitle = "n = 100, p = 0.5") +
  theme_minimal()
```


## Explore Binomial Graphs

- The following arrangement of graphs shows how the shape of binomial distributions changes for $p \in \{0.1, 0.5, 0.9\}$ as $n$ increase from 1 to 1024 by powers of 2.
- Notice how the skewness and scale change for each $p$ as $n$ increases.
- Install the **egg** library for ``ggarrange() ` to arrange the plots
- Note the use of:
    - the **tidyr** function `expand_grid()` which creates a data frame with all possible combinations of `n_options` and `p_options`
    - the use of `pmap()` from **purrr** to iterate over the rows of a data frame
    - the anonymous function which produces a plot where `.x` is the first argument `n_options` and `.y` is the second argument `p_options`

```{r binomial-graphs, echo = TRUE, fig.height = 24}
n_options = 2^seq(0, 10, by = 1)
p_options = c(0.1, 0.5, 0.9)

binomial_plots = expand_grid(n_options, p_options) %>% 
  pmap(~{
    gbinom(.x, .y,
           color = if_else(.y == 0.1, "red", 
                           if_else(.y == 0.5, "purple", "blue")),
           scale = TRUE) +
      theme_minimal()
  })

ggarrange(plots = binomial_plots,
          ncol = length(p_options),
          nrow = length(n_options))
```

- When $p = 0.1$, the distributions are skewed for small $n$, but become more symmetric as $n$ increases.
- When $p=0.5$, the distributions are always symmetric.
- The same shape emerges in both cases: the normal bell-shaped curve.

## Binomial Probabilities

- The formula for binomial probabilities is given here.

$$
P(X = k) = \binom{n}{k} p^k(1-p)^{n-k}, \qquad \text{for $k=0,1,\ldots,n$}
$$

- The binomial coefficient counts the number of distinct sequences of length $n$ with exactly $k$ symbols of one type and $n-k$ symbols of another.

$$
\binom{n}{k} = \frac{n!}{k!(n-k)!}
$$

- The expression $p^k (1-p)^{n-k}$ is the probability of each single sequence of length $n$ with exactly $k$ symbols of one type and $n-k$ symbols of another.

- The probability that there are exactly $k$ "successes" in a sequence of $n$ independent trials with success probability $p$ is the sum of the probabilities of all the ways this could occur.

### Example

- The R function `choose()` calculates $\binom{n}{k}$
- The R function `factorial()` calculates $n!$
    - This calculation is not stable for moderately large values.
    - A more stable option is `lgamma()` where `lgamma(x+1)` is equal to $\ln x!$.
- But we will use the built-in function `dbinom()` to calculate binomial probabilities without worrying about the formula or numerical instabilities.    

```{r binomial-calc}
n = 5
p = 0.5
k = 2

choose(n, k)
factorial(n)/(factorial(k)*factorial(n-k)) # n! /(k!(n-k)!)

choose(n, k)*p^k*(1-p)^(n-k)
dbinom(k,n,p) # see below
```


## R Binomial Functions

- `rbinom(n, size, prob)`
- `dbinom(x, size, prob)`
- `pbinom(q, size, prob)`
- `qbinom(p, size, prob)`

- Note that with these functions, the parameters for the Binomial are `size` and `prob`
(what we've been referring to as $n$ and $p$, respectively).
- The argument `n` in `rbinom()` is the sample size, how many random variables for which to generate values.

### Demonstrations

#### rbinom()

```{r rbinom}
# Simulate binomial random variables
n = 5
p = 0.5
x = rbinom(6, n, p)
x
mean(x) # estimate based on sample
n*p # true mean

var(x) # estimate based on sample
n*p*(1-p) # true variance
```

#### dbinom()

```{r dbinom}
## Binomial "density" calculations
dbinom(0:n, n, p)

dbinom(4, n, p)
```

#### gbinom()

- This is not a base R command, but is defined in `ggprob.R`.

```{r dbinom-plot}
## Binomial "density" plot
gbinom(n,p) 
```

#### pbinom()

```{r pbinom}
## Binomial distribution calculations
## P(X <= x) = F(x), where F is the distribution function.

# P(X <= 3):
pbinom(3, n, p)
dbinom(0, n, p) + dbinom(1, n, p) + dbinom(2, n, p) + dbinom(3, n, p)
1 - dbinom(4, n, p) - dbinom(5, n, p)

# P(X > 3):
1 - pbinom(3, n, p) # 1 - P(X <= 3)
pbinom(3, n, p, lower.tail=FALSE) # P(X > 3)
dbinom(4, n, p) + dbinom(5, n, p)

# P(X < 3):
pbinom(3, n, p) - dbinom(3, n, p)
pbinom(2, n, p) # P(X <= 2) = P(X < 3)
```

#### qbinom()

```{r qbinom}
## Binomial quantile calculations
## Docs:  The quantile is defined as the smallest value x such that P(X<=x) = F(x) ≥ p, where F is the distribution function.
qbinom(.2, n, p) # which x such that P(X <= x) = 0.2; there may not be an exact x
dbinom(0, n, p) + dbinom(1, n, p) + dbinom(2, n, p)
dbinom(0, n, p) + dbinom(1, n, p) 
```

## Problems

### Problem 1

What are the mean and standard deviation of the $\text{Binomial}(90, 0.7)$ distribution?

#### Solution

The mean is $\mu = (90)(0.7) = `r 90*0.7`$.

The standard deviation is $\sigma = \sqrt{90(0.7)(0.3)} = `r sqrt(90*0.7*0.3)`$.

### Problem 2

Plot the $\text{Binomial}(90, 0.7)$ distribution. Add a red partly transparent solid vertical line at the mean, dashed vertical lines one standard deviation above and below the mean, and dotted lines two standard deviations above and below the mean.

```{r}
n = 90
p = 0.7
mu = n*p
sigma = sqrt(n*p*(1-p))

gbinom(n, p, scale = TRUE) +
  geom_vline(xintercept = mu, color = "red", alpha = 0.5) +
  geom_vline(xintercept = mu + c(-1,1)*sigma,
             color = "red", linetype = "dashed") +
  geom_vline(xintercept = mu + c(-2,2)*sigma,
             color = "red", linetype = "dotted") +
  theme_minimal()
```

### Problem 3

If $X \sim \text{Binomial}(90, 0.7)$, what is $P(X = \mu)$?

#### Solution

```{r}
dbinom(mu, n, p)
```

### Problem 4

If $X \sim \text{Binomial}(90, 0.7)$,
what are $P(\mu - \sigma \le X \le \mu + \sigma)$ and 
$P(\mu - 2\sigma \le X \le \mu + 2\sigma)$?

#### Solution

```{r}
## endpoints
mu + c(-2,-1,0,1,2)*sigma

## Within one standard deviation
## P(mu - sigma <= X <= mu + sigma)
## P(59 <= X <= 67) = P(X <= 67) - P(X <= 58)
pbinom(67, n, p) - pbinom(58, n, p)

## Sum of probabilities
sum( dbinom(59:67, n, p) )

## Within two standard deviations
## P(mu - 2*sigma <= X <= mu + 2*sigma)
## P(55 <= X <= 71) = P(X <= 71) - P(X <= 54)
pbinom(71, n, p) - pbinom(54, n, p)

## Sum of probabilities
sum( dbinom(55:71, n, p) )
```

### Problem 5

Find the 0.05 and 0.95 quantiles of the $\text{Binomial}(90, 0.7)$ distribution.

```{r}
qbinom(c(0.05, 0.95), n, p)

## Compare to the tail probabilities
## P(X <= 56) >= 0.05 and P(X >= 56) >= 1 - 0.05 = 0.95
pbinom(56, n, p)
1 - pbinom(55, n, p)

## Compare to the tail probabilities
## P(X <= 70) >= 0.95 and P(X >= 70) >= 1 - 0.95 = 0.05
pbinom(70, n, p)
1 - pbinom(69, n, p)
```

### Problem 6

Explain why each of the following random variables does not have a binomial distribution.

### A

$X_1$ is the number of times a fair coin is tossed until we have observed at least one head and one tail.

#### Solution

There is not a fixed number of trials.

### B

$X_2$ is the number of combined number of red balls drawn when one is picked from a bucket with 10% red balls, a second is picked from a bucket with 20% red balls, and a third is drawn from a bucket with 30% red balls.

#### Solution

The value of $p$ is not the same for all trials.

### C

$X_3$ is the number of red balls selected in a sample of 5 chosen without replacement from a bucket with ten red balls and ten white balls.

#### Solution

The trials are not independent. The color of the first ball selected affects the probability of the color of the second ball, for example.

### Problem 7

Make a trace plot of the 0.75 quantile of binomial distributions with $p=0.7$ and $n$ varying from 1 to 500 with $n$ on the x axis and the quantile on the y axis.
Describe the shape of the relationship between these variables.

#### Solution

```{r}
prob7 = tibble(
  n = 1:500,
  q75 = qbinom(0.75, n, 0.7))

ggplot(prob7, aes(x=n, y=q75)) +
  geom_line()
```

- The relationship is approximately linear.

### Problem 7.5

If $q_{0.75}(n)$ is the 0.75 quantile of the binomial distribution
with $p = 0.7$, define $z = (q_{0.75}(n) - np)/\sqrt{np(1-p)}$.
(Subtract the binomial mean and divide by the binomial standard deviation.)
Make a traceplot of $z$ on the y axis and $n$ varying from 1 to 500 on the x axis. Add a straight regression line to the plot. Describe the plot.

#### Solution

```{r}
prob7 = prob7 %>% 
  mutate(mu = n*0.7,
         sigma = sqrt(n*0.7*0.3),
         z = (q75 - mu)/sigma)

ggplot(prob7, aes(x=n, y = z)) +
  geom_line() +
  geom_smooth(method = "lm")
```

- The pattern oscillates around a nearly horizontal line with the size of the oscillations decreasing as $n$ increases.

```{r}
prob7 %>% 
  summarize(z = mean(z))
```

- The mean value of $z$ is approximately 0.68.

### Problem 8

For $n$ varying from 2 to 1000 by 2, and $p = 0.5$, calculate $P(X = n/2)$.

### A

Graph the probabilities versus $n$ and describe the relationship.

```{r}
prob8 = tibble(
  n = seq(2,1000,by = 2),
  prob = dbinom(n/2, n, 0.5)
)

ggplot(prob8, aes(x = n, y = prob)) +
  geom_line()
```

- The line quickly drops and approaches a limit at zero.

### B

Repeat, but plot both variables on the natural log scale.

```{r}
ggplot(prob8, aes(x = n, y = prob)) +
  geom_line() +
  scale_x_continuous(trans = "log", breaks = c(5, 10, 50, 100, 500)) +
  scale_y_continuous(trans = "log", breaks = c(0.05, 0.1, 0.2, 0.3, 0.4))
  
```

- The relationship is approximately linear for $n$ not too small.



